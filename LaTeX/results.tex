\section{Results}\label{sec:results}
\setlength{\abovecaptionskip}{0pt plus 0pt minus 2pt}

\begin{figure}[t!]
\centering
\captionsetup{justification=centering,font=scriptsize}
\begin{subfigure}{0.4\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\setlength\fwidth{0.85\textwidth}
	\setlength\fheight{120}
	\input{./figures/decoding_time.tex}
	\caption{Average time to decode a packet vs absolute redundancy}
	\label{fig:decoding_time}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.4\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\setlength\fwidth{0.85\textwidth}
	\setlength\fheight{120}
	\input{./figures/decoding_time_rel.tex}
	\caption{Average time to decode a packet vs relative redundancy}
	\label{fig:decoding_time_rel}
\end{subfigure}
\caption{Average time to decode a packet, , for different $K$. Notice that y axis is in logarithmic scale}
\label{fig:dc_all}
\end{figure}

In our experiments to evaluate the performances of the system we have considered the transmission of a file of size 46.2 MB. Packet's header consists in 32 bits of pRNG seed plus 8 bits of block ID, for a total header size of 40 bits. The ACK contains 32 bits to represent the number of packets the receiver still needs to get $N$ correct packets (therefore we allow for a value of $N$ up to $2^{32}\simeq 4\cdot 10^9$, which is more than enough for our purposes) plus 8 bits of block ID, again for a total of 40 bits. The various experimental scenarios have been summarized in Table 1.

\begin{center}
\begin{tabular}{ccc}

\toprule
scenario & payload (byte) & minimum timeout\\
 & & before RTX (ms) \\
\midrule
localhost aggressive&2048&1\\
localhost aggressive, PER estimation & 2048 & 1 \\
localhost efficient&2048&100\\
WiFi&1467&100\\
Network&1467&100\\
\bottomrule
\end{tabular}
\captionof{table}{Experimental setups}
\end{center}
The payload size of WiFi and network is dictated by the MTU size. \\
We remark that with these setting we are able to transmit a file of size up to 2.25 PB in localhost and 1.61 PB over WiFi or network (if we have no redundancy and we adjust the number of bytes used to communicate the file size).\MP{do we actually have to specify?}\\
The WiFi experiment was carried out using two different setups. In the \textit{high SNR} scenario sender and router, router and receiver are at a distance $d= 3$ m, with one brick wall between sender and router. The \textit{low SNR} scenario deployed sender and router in the same positions, but the receiver was moved 2 walls and 15 m away from the router. The WiFi router was a 802.11a/b/g device. 
The transmission over the network has been carried out from Padua to Lausanne. The sender was connected via WiFi to a FTTC connection with 10 Mbits in uplink, while the receiver was connected via Ethernet on a symmetric connection of 100 Mbits (therefore the bottleneck is the 10 Mbits uplink of the sender). For localhost and network experiments, the receiver was executed on a workstation with a 3.4 GHz i7 processor and 8 GB of RAM, while in both WiFi setups the laptop on which the receiver run had a 2.2 GHz i7 processor with 8 GB of RAM.

In all these regimes we consider a redundancy for which the probability of decoding failures (i.e. the receiver correctly receives $N$ packets, but they are not sufficient to decode the $K$ informative packets) is very low (typically $<10^{-3}$), and therefore negligible. \\
The first very important trade-off is between decoding time and transmission time as a function of the redundancy, which translates in most of the cases in a trade-off between goodput and efficiency. In fact, for a fixed $K$ and PER, adding redundancy decreases the time to decode (for our cases of interest), but it also increases the time to transmit the file (because there are more packets to transmit). In general we can write that $delay=transmission\_time+decoding\_time$ where \emph{transmission\_time} includes all time spent in propagation, waiting, retransmitting, etc. over the channel, and \emph{decoding\_time} is the time that it takes to the decoder to decoded a block of $N$ packets once they have all been correctly received. We also remark that transmission time slightly decreases when $K$ increases. In fact packets of the same block are transmitted using a system similar to SR (the receiver tells the sender how many packets it needs, and the sender sends them), while blocks are transmitted in a S\&W fashion (i.e. a block is transmitted only after the previous one has been successfully decoded). As a consequence we have that we have that delay increases with the number of blocks. We can sum up these considerations in Table 2, where we show the dependencies of \emph{transmission\_time} and \emph{decoding\_time} on one parameter by keeping all others fixed.
\begin{center}
\begin{tabular}{c|c|c|c|c}

\toprule
&$K$&$N-K$&$\frac{N}{K}$&PER\\
\midrule
transmission\_time&$\searrow$& & $\nearrow$&$\nearrow$\\ \hline
decoding\_time&$\nearrow$&$\sim \searrow$&&independent\\ 
\bottomrule
\end{tabular}
\captionof{table}{Monotonicities}
\end{center}

In Fig. \ref{fig:dc_all} we show the decoding time (localhost scenario) for various\MP{ a set of } values of $K$ and redundancy. Clearly, in general this time decreases as we increase redundancy, since there are more connections in the graph of message passing, and therefore at each iteration we are able to resolve more nodes. Hence less iterations are needed to decode the packets. However, as we can see from Fig.\ref{fig:decoding_time_rel}, with the redundancy above a certain value\MP{ threshold } the time to read and write the graph for message passing (which increases with $N$) becomes dominant over the time (number of iterations) needed for message passing itself, and therefore the decoding time increases. However this increase is much less significant than the decrease at low redundancies, and for practical purposes we can assumes that decoding time for a certain value on stays constant. Roughly speaking, we can see that around a value of relative redundancy $\frac{N}{K}=1.5$ there is a change in regime: the derivative of the decoding time according to redundancy changes dramatically. In particular as we approach 1 the increase in decoding time is exponential. We also remark (in part by guessing the behavior of the lines if we were able to explore the whole interval of redundancies for all values of $K$) that, if we fix a given relative redundancy, the decoding time increases with $K$.\\

\begin{figure}[t!]
\centering
\begin{subfigure}{0.24\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_goodput_1000.tex}
	\caption{$K=1000$}
	\label{fig:lh_good_1000}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_goodput_3500.tex}
	\caption{$K=3500$}
	\label{fig:lh_good_3500}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_goodput_5500.tex}
	\caption{$K=5500$}
	\label{fig:lh_good_5500}
\end{subfigure}
\caption{Goodput for $K \in \{1000, 3500, 5500\}$ in localhost, aggressive setup}
\label{fig:goodput_aggr}
\end{figure}

\begin{figure}[t!]
\centering
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_efficiency_1000.tex}
	\caption{$K=1000$}
	\label{fig:lh_eff_1000}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_efficiency_3500.tex}
	\caption{$K=3500$}
	\label{fig:lh_eff_3500}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_efficiency_5500.tex}
	\caption{$K=5500$}
	\label{fig:lh_eff_5500}
\end{subfigure}
\caption{Efficiency for $K \in \{1000, 3500, 5500\}$ in localhost, aggressive setup}
\label{fig:efficiency_aggr}
\end{figure}

\begin{figure}[t!]
\centering
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_PM1_goodput_3500.tex}
	\caption{Goodput for $K=3500$, using PER estimation}
	\label{fig:pm1_goodput}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_PM1_efficiency_3500.tex}
	\caption{Efficiency for $K=3500$, using PER estimation}
	\label{fig:pm1_efficiency}
\end{subfigure}
\caption{Goodput and efficiency for $K=3500$, using PER estimation, aggressive setup}
\label{fig:pm1}
\end{figure}

This trade-off between transmission time and decoding time is very well shown in Figures \ref{fig:goodput_aggr} and \ref{fig:efficiency_aggr}, which show the performance obtained in localhost with an aggressive ARQ (i.e. minimum timeout before retransmission). First of all we remark that the goodput (i.e., useful bit per second) decreases approximatively linearly with the packet error probability. Moreover, for $K$ small, adding redundancy decreases the goodput. In fact for $K$ small and redundancy starting at $N-K=1000$ (i.e. 100\%), transmission time prevails over decoding time, which, at these levels of redundancy, increases slowly as a function of $N$. Therefore, since the transmission time increases with redundancy (we can assume that transmission time increases approximatively linearly in $N$), the overall delay increases. Hence, the goodput decreases by adding redundancy.
On the other hand, if we increase $K$, we reduce the relative redundancy. From \ref{fig:decoding_time_rel} we see that for $K=3500$ or $K=5500$, decoding time dramatically depend by redundancy (and also increases with $K$), and therefore when redundancy in small the time to decode the file has more impact than the time to transmit it. Hence, in these cases the goodput increases by adding redundancy.  As a consequence, if efficiency is not to be optimized, it is better, for $K$ big enough, to choose high redundancy in order to decode faster, rather than reduce $N$ to transmit faster.\\
We also observe that goodput decreases with $K$. In fact in general in localhost, since transmission time is small, for a fixed absolute redundancy, increasing $K$ allows to marginally improve the time to transmit (fewer packets to transmit because the relative redundancy is lower), while it significantly increases the time to decode (because the relative redundancy decreases). \\
However the efficiency, i.e. the ratio between information and sent packets, always decreases with redundancy (since we are not taking into account the time to transmit the packets).\\
We also remark that the difference in goodput for a given $K$ and two values of $N$ decreases as PER increases. In fact we can write, for $N>N'$ and $PER>PER'$
\begin{equation*}
\begin{split}
[delay(N,PER')-delay(N',PER')]-[delay(N,PER)-delay(N',PER)]=\\
=[tx\_time(N,PER')-tx\_time(N,PER)]-[tx\_time(N',PER')-tx\_time(N',PER)]
\end{split}
\end{equation*}
because the decoding time does not depend on PER. If we assume that the transmission time is a linear function of the number of retransmissions, and that the number of packets left to transmit is a geometric sequence in the number of retransmission $r$, we can write $N\cdot PER^r=c \Rightarrow r=-\frac{\ln(N)-\ln(c)}{\ln(PER)}$, where $c$ is a small constant at which we consider that all packets have been received (e.g. $c=0.1$ \MP{what about: e.g. $c=1$ so that we can neglect the term $\ln(c)$}). Therefore
\begin{equation*}
\begin{split}
[delay(N,PER')-delay(N',PER')]-[delay(N,PER)-delay(N',PER)]=\\ 
=\alpha\frac{\ln(N)}{\ln PER \ln PER'}\ln{\frac{PER'}{PER}}-\alpha\frac{\ln(N')}{\ln PER \ln PER'}\ln{\frac{PER'}{PER}}=\frac{\alpha}{\ln PER \ln PER'}\ln{\frac{PER'}{PER}}\ln{\frac{N}{N'}}<0
\end{split}
\end{equation*}
since $N>N'$ and $1>PER>PER'>0$. Therefore the gap in delay increases as PER increases, and since the goodput depends on the inverse of the delay, the gap in goodput decreases as PER increases.
\MP{
	Why $K$? For the delay, if we have 1 + r transmissions, $D(N, PER) = N(1 - \frac{\ln(N)}{\ln(PER)})$. In this case, the final formula is
	$$
		\ln\frac{P'}{P}\frac{1}{\ln P' \ln P} \left( N\ln N - N'\ln N' \right) < 0
	$$
	Anyway, if there is enough space I would add some of the steps to the solution, to make it simpler to understand
}
\SO{check following section on pm1}
In Fig. \ref{fig:pm1} we show the result we obtain by enabling the PER estimation mode. In this mode, the sender tries to estimate the PER, in order to improve system performances. We see that effectively we have quite a significant increase in goodput, while the efficiency remains approximately the same (except for the case $PER=0$, which gives some estimation troubles, probably due to convergence problems of estimators). Therefore we have that our PER estimation method proves to be quite useful, even though performances don't change dramatically.\\  
\MP{Something on efficiency oriented setup? For example: In the efficiency oriented setup the minimum timeout for packet gap detection is set to 100 ms. It can be seen that efficiency slightly increases, but the difference is minimal, due to limitations of the localhost experiment (?). The goodput however shows a different behavior for $PER \ge 0.1$... I can add something more tomorrow}
In Figures \ref{fig:goodput_nonaggr} and  \ref{fig:eff_nonaggr} we show results for the efficiency-oriented setup, where the minimum timeout for packet gap detection is increased from 1 ms to 100 ms. As expected, goodput is lower with respect to the aggressive scenario, since we are increasing the delay (with the exception of the case PER=0, for which, in fact, no packets are lost, and therefore there are no retransmissions and this timeout has little impact of the performance). We also see that efficiency only improves marginally. An hypothesis for that is that in localhost transmission time for a packet is less than 1 ms, therefore by increasing the minimum timeout we are not significantly decreasing the number of packets we retransmit, and therefore efficiency remains more or less the same. 
% TODO NET_localhost: goodput, efficiency with PER on x axis $K$ 1000 3500 5500, 3 plot per row, 2 rows
% Relation between different parameters (K, N) and goodput/efficiency. Tradeoff redundancy goodput time to decode.
% Compare the two setup and show the tradeoff between goodput and efficiency, according to the different timeout settings.
% Something on useless packets (i.e. packets that arrive after the completion of a decoding attempt, that may be sent after 
% the reception of previous ACKs), notice that they tend to 0.

\begin{figure}[!hp]
\centering
\begin{subfigure}{0.24\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_goodput_1000_100ms.tex}
	\caption{$K=1000$}
	\label{fig:lh_good_1000_eff}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_goodput_3500_100ms.tex}
	\caption{$K=3500$}
	\label{fig:lh_good_3500_eff}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_goodput_5500_100ms.tex}
	\caption{$K=5500$}
	\label{fig:lh_good_5500_eff}
\end{subfigure}
\caption{Goodput for $K \in \{1000, 3500, 5500\}$ in localhost, efficiency oriented setup}
\label{fig:goodput_nonaggr}
\end{figure}

\begin{figure}[!hp]
\centering
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_efficiency_1000_100ms.tex}
	\caption{$K=1000$}
	\label{fig:lh_eff_1000_eff}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_efficiency_3500_100ms.tex}
	\caption{$K=3500$}
	\label{fig:lh_eff_3500_eff}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_efficiency_5500_100ms.tex}
	\caption{$K=5500$}
	\label{fig:lh_eff_5500_eff}
\end{subfigure}
\caption{Efficiency for $K \in \{1000, 3500, 5500\}$ in localhost, efficiency oriented setup}
\label{fig:eff_nonaggr}
\end{figure}

\begin{figure}[!hp]
\centering
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/wifi_eff.tex}
	\caption{Efficiency}
	\label{fig:wifi_eff}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/wifi_goodput.tex}
	\caption{Goodput}
	\label{fig:wifi_good}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/wifi_throughput.tex}
	\caption{Throughput}
	\label{fig:wifi_thr}
\end{subfigure}
\caption{Metrics for WiFi simulation. }
\label{fig:wifi}
\end{figure}

\begin{figure}[!hp]
\centering
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/wifi_eff_lsnr_3500.tex}
	\caption{Efficiency}
	\label{fig:wifi_eff_lsnr_3500}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/wifi_goodput_lsnr_3500.tex}
	\caption{Goodput}
	\label{fig:wifi_good_lsnr_3500}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/wifi_throughput_lsnr_3500.tex}
	\caption{Throughput}
	\label{fig:wifi_thr_lsnr_3500}
\end{subfigure}
\caption{Comparison between setup with \textit{high SNR} and setup with \textit{low SNR}, for $K=3500$}
\label{fig:wifi_lsnr_3500}
\end{figure}
\begin{figure}[!hp]
\centering
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/net_eff.tex}
	\caption{Efficiency}
	\label{fig:net_eff}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/net_goodput.tex}
	\caption{Goodput}
	\label{fig:net_good}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/net_throughput.tex}
	\caption{Throughput}
	\label{fig:net_thr}
\end{subfigure}
\caption{Metrics for Net simulation}
\label{fig:net}
\end{figure}
\begin{figure}[!h]
\centering
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_eff_LT3500_RF.tex}
	\caption{Efficiency}
	\label{fig:RF_eff}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\centering
	\setlength\fwidth{\textwidth}
	\setlength\fheight{140}
	\input{./figures/localhost_goodput_LT3500_RF.tex}
	\caption{Goodput}
	\label{fig:RF_good}
\end{subfigure}\hspace{2em}%
\begin{subfigure}{0.23\textwidth}
	\captionsetup{justification=centering,font=scriptsize}
	\small
	\centering
	\begin{tabular}[c]{c|c}
		LT [ms] & RF [ms] \\ \hline
		0.15 & 0.61 \\
	\end{tabular}
	\caption{Decoding time per packet}
	\label{fig:RFLT_dectime}
\end{subfigure}
\caption{Comparison between LT, $K=3500$ \& $N=4500$, and RF, $K=12$ \& $N=17$}
\label{fig:RFLT}
\end{figure}


% 3 pages
% wifi: goodput and efficiency, with x axis with $N$ - K, also throughput, show that for N-K small the decoding time is more significant than rx time. 
Again, in Fig. \ref{fig:wifi} we see the trade-off between goodput and efficiency. Efficiency is always monotonic with respect to redundancy, whereas the behavior of the goodput is more complicated, and depends on which of the transmission time and the decoding time prevails. In particular we see that goodput increases dramatically for high $K$ when we increase redundancy from 1000 to 1500. This, again, is due to the fact that too little redundancy (in ratio with respect to K) yields longer decoding times, which can dominate over the transmission time, and therefore having little redundancy degrades the performance. This explains the peak at $N-K=1500$ for high $K$: at lower redundancy performance is dictated by decoding time (which increases as redundancy decreases), whereas for higher redundancy performance depends mainly on transmission time (which increases as redundancy increases, since we have more packets to transmit\MP{, which is not trivial in non localhost scenarios}). Therefore goodput depends both on an increasing and decreasing function with respect to redundancy, which explains the non-monotonicity.  We also remark that clearly throughput increases with redundancy, since we reduce the time to decode, whereas the time to transmit the single packets doesn't change very much (if the transmission rate of the packet is high enough, which is the case for Wi-Fi 802.11g which is approximatively 50 Mbit/s). This is confirmed by the fact that the increase in throughput is greater for low $K$, because we are adding in percentage more redundancy, which greatly increase the decoding time. 

% $K$ = 3500, possibly others, show the difference between a nice setting and a low snr setting
In Figure \ref{fig:wifi_lsnr_3500} we show the experimental results obtained using WiFi in a \textit{high SNR} scenario and \textit{low SNR} for $K=3500$ and different $N-K$. Even though experimental measurements show quite a large variance (as can be noted by observing the interval of confidence), it appears quite clearly that, as expected, in the high SNR scenario we obtain better performances with respect to the low SNR case. Notice also that the gap between the throughput mean values is greater than the gap between goodput values. However measurement uncertainties do not allow to draw further and more precise conclusions (even though we can assume that the qualitative behavior does not differ very much from the other cases, and the same considerations apply).

Fig. \ref{fig:net} shows the results of packet transmission between Padova and Lausanne. Also here we see the same qualitative behavior of the other cases. In particular we remark the crossing of the goodput curves for $K=3500$ and $K=4500$ due to the fact that when redundancy is too small, the time to decode a block increases significantly and impacts on the performances (even though less with respect to the Wi-Fi scenario since in this scenario the transmission time is higher, since we have to transmit a packet through a network of approximatively 12 hops, and the decoding is run on a faster workstation).
% net: goodput and efficiency, with x axis with $N$ - K, also throughput, show that for N-K small the decoding time is more significant than rx time. 


% comparison between RF and LT in localhost
% TODO decoding time comparison
Fig. \ref{fig:RFLT} shows a choice of $N$ and $K$ which yield approximately the same efficiency for Random Fountain and LT, but allow for a much bigger goodput with LT, proving therefore the usefulness of LT. In fact, using RF, we are constrained to low values of $N$ and $K$ (respectively 17 and 12) because of the complexity of the Gau{\ss} elimination. Low values of $K$ oblige to a greater relative redundancy to have low possibility of failing the decode, and moreover result in more blocks (of $K$ packets). Furthermore, we have that decoding time for RF is bigger than for LT, even if $K$ and $N$ are much smaller, confirming the fact that the LT decoding algorithm is much faster than Gau{\ss} elimination. All this implies a bigger delay. Hence, the goodput decreases.
